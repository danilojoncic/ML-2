{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNsgLUxQJqmwwdbuTVhwKyX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zv5NIX0ZrV74","executionInfo":{"status":"ok","timestamp":1717193477805,"user_tz":-120,"elapsed":75022,"user":{"displayName":"Danilo Joncic","userId":"12467872683224926268"}},"outputId":"18080688-ffe9-4238-e713-dfc042af326d"},"outputs":[{"output_type":"stream","name":"stdout","text":["   Nitrogen  Phosphorus  Potassium  Temperature   Humidity  pH_Value  \\\n","0        90          42         43    20.879744  82.002744  6.502985   \n","1        85          58         41    21.770462  80.319644  7.038096   \n","2        60          55         44    23.004459  82.320763  7.840207   \n","3        74          35         40    26.491096  80.158363  6.980401   \n","4        78          42         42    20.130175  81.604873  7.628473   \n","\n","     Rainfall  Crop  \n","0  202.935536  Rice  \n","1  226.655537  Rice  \n","2  263.964248  Rice  \n","3  242.864034  Rice  \n","4  262.717340  Rice  \n","Nitrogen       0\n","Phosphorus     0\n","Potassium      0\n","Temperature    0\n","Humidity       0\n","pH_Value       0\n","Rainfall       0\n","Crop           0\n","dtype: int64\n","Epoch: 1/100, Avg loss: 56.29970\n","Epoch: 2/100, Avg loss: 4.97440\n","Epoch: 3/100, Avg loss: 3.60748\n","Epoch: 4/100, Avg loss: 4.01247\n","Epoch: 5/100, Avg loss: 2.69790\n","Epoch: 6/100, Avg loss: 1.66037\n","Epoch: 7/100, Avg loss: 1.33280\n","Epoch: 8/100, Avg loss: 1.52353\n","Epoch: 9/100, Avg loss: 2.34596\n","Epoch: 10/100, Avg loss: 1.99528\n","Epoch: 11/100, Avg loss: 1.76651\n","Epoch: 12/100, Avg loss: 0.78141\n","Epoch: 13/100, Avg loss: 1.09661\n","Epoch: 14/100, Avg loss: 1.03273\n","Epoch: 15/100, Avg loss: 1.80889\n","Epoch: 16/100, Avg loss: 1.10293\n","Epoch: 17/100, Avg loss: 2.00811\n","Epoch: 18/100, Avg loss: 2.15264\n","Epoch: 19/100, Avg loss: 1.29864\n","Epoch: 20/100, Avg loss: 0.91173\n","Epoch: 21/100, Avg loss: 0.67755\n","Epoch: 22/100, Avg loss: 0.29968\n","Epoch: 23/100, Avg loss: 0.68801\n","Epoch: 24/100, Avg loss: 0.66959\n","Epoch: 25/100, Avg loss: 1.81160\n","Epoch: 26/100, Avg loss: 1.61801\n","Epoch: 27/100, Avg loss: 1.47199\n","Epoch: 28/100, Avg loss: 2.03633\n","Epoch: 29/100, Avg loss: 1.84443\n","Epoch: 30/100, Avg loss: 1.13393\n","Epoch: 31/100, Avg loss: 0.76237\n","Epoch: 32/100, Avg loss: 1.66626\n","Epoch: 33/100, Avg loss: 1.72341\n","Epoch: 34/100, Avg loss: 0.26808\n","Epoch: 35/100, Avg loss: 0.27281\n","Epoch: 36/100, Avg loss: 0.13271\n","Epoch: 37/100, Avg loss: 0.20402\n","Epoch: 38/100, Avg loss: 0.18848\n","Epoch: 39/100, Avg loss: 0.15391\n","Epoch: 40/100, Avg loss: 0.08048\n","Epoch: 41/100, Avg loss: 0.07558\n","Epoch: 42/100, Avg loss: 0.07015\n","Epoch: 43/100, Avg loss: 0.06481\n","Epoch: 44/100, Avg loss: 0.06799\n","Epoch: 45/100, Avg loss: 0.43155\n","Epoch: 46/100, Avg loss: 0.03219\n","Epoch: 47/100, Avg loss: 0.25635\n","Epoch: 48/100, Avg loss: 0.19099\n","Epoch: 49/100, Avg loss: 0.23502\n","Epoch: 50/100, Avg loss: 1.09856\n","Epoch: 51/100, Avg loss: 0.93827\n","Epoch: 52/100, Avg loss: 0.63390\n","Epoch: 53/100, Avg loss: 0.33331\n","Epoch: 54/100, Avg loss: 0.13915\n","Epoch: 55/100, Avg loss: 0.11895\n","Epoch: 56/100, Avg loss: 0.74841\n","Epoch: 57/100, Avg loss: 1.18671\n","Epoch: 58/100, Avg loss: 3.26439\n","Epoch: 59/100, Avg loss: 3.43582\n","Epoch: 60/100, Avg loss: 1.46917\n","Epoch: 61/100, Avg loss: 0.68479\n","Epoch: 62/100, Avg loss: 0.62907\n","Epoch: 63/100, Avg loss: 0.33591\n","Epoch: 64/100, Avg loss: 0.51381\n","Epoch: 65/100, Avg loss: 0.07679\n","Epoch: 66/100, Avg loss: 0.14415\n","Epoch: 67/100, Avg loss: 0.23655\n","Epoch: 68/100, Avg loss: 0.26359\n","Epoch: 69/100, Avg loss: 0.00000\n","Epoch: 70/100, Avg loss: 0.06136\n","Epoch: 71/100, Avg loss: 0.09399\n","Epoch: 72/100, Avg loss: 0.00000\n","Epoch: 73/100, Avg loss: 0.00000\n","Epoch: 74/100, Avg loss: 0.00000\n","Epoch: 75/100, Avg loss: 0.00000\n","Epoch: 76/100, Avg loss: 0.00000\n","Epoch: 77/100, Avg loss: 0.00000\n","Epoch: 78/100, Avg loss: 0.00000\n","Epoch: 79/100, Avg loss: 0.00000\n","Epoch: 80/100, Avg loss: 0.00000\n","Epoch: 81/100, Avg loss: 0.00000\n","Epoch: 82/100, Avg loss: 0.00000\n","Epoch: 83/100, Avg loss: 0.00000\n","Epoch: 84/100, Avg loss: 0.00000\n","Epoch: 85/100, Avg loss: 0.00000\n","Epoch: 86/100, Avg loss: 0.00000\n","Epoch: 87/100, Avg loss: 0.00000\n","Epoch: 88/100, Avg loss: 0.00000\n","Epoch: 89/100, Avg loss: 0.00000\n","Epoch: 90/100, Avg loss: 0.00000\n","Epoch: 91/100, Avg loss: 0.00000\n","Epoch: 92/100, Avg loss: 0.00000\n","Epoch: 93/100, Avg loss: 0.00000\n","Epoch: 94/100, Avg loss: 0.00000\n","Epoch: 95/100, Avg loss: 0.00000\n","Epoch: 96/100, Avg loss: 0.00000\n","Epoch: 97/100, Avg loss: 0.00000\n","Epoch: 98/100, Avg loss: 0.00000\n","Epoch: 99/100, Avg loss: 0.00000\n","Epoch: 100/100, Avg loss: 0.00000\n","Test accuracy: 0.980\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","\n","data = pd.read_csv('crop.csv')\n","\n","print(data.head())\n","\n","print(data.isnull().sum())\n","\n","data = data.dropna()\n","\n","encoder = LabelEncoder()\n","data['Crop'] = encoder.fit_transform(data['Crop'])\n","\n","X = data[['Nitrogen', 'Phosphorus', 'Potassium', 'Temperature', 'Humidity', 'pH_Value', 'Rainfall']]\n","y = data['Crop']\n","\n","X = (X - X.mean()) / X.std()\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","learning_rate = 0.01\n","nb_epochs = 100\n","batch_size = 32\n","\n","nb_input = X_train.shape[1]\n","nb_hidden1 = 256\n","nb_hidden2 = 256\n","nb_classes = len(np.unique(y))\n","\n","X_train = X_train.to_numpy()\n","X_test = X_test.to_numpy()\n","y_train = tf.keras.utils.to_categorical(y_train, nb_classes)\n","y_test = tf.keras.utils.to_categorical(y_test, nb_classes)\n","\n","w = {\n","    '1': tf.Variable(tf.random.normal([nb_input, nb_hidden1], dtype=tf.float64)),\n","    '2': tf.Variable(tf.random.normal([nb_hidden1, nb_hidden2], dtype=tf.float64)),\n","    'out': tf.Variable(tf.random.normal([nb_hidden2, nb_classes], dtype=tf.float64))\n","}\n","\n","b = {\n","    '1': tf.Variable(tf.random.normal([nb_hidden1], dtype=tf.float64)),\n","    '2': tf.Variable(tf.random.normal([nb_hidden2], dtype=tf.float64)),\n","    'out': tf.Variable(tf.random.normal([nb_classes], dtype=tf.float64))\n","}\n","\n","f = {\n","    '1': tf.nn.relu,\n","    '2': tf.nn.relu,\n","    'out': tf.nn.softmax\n","}\n","\n","def runNN(x):\n","    z1 = tf.add(tf.matmul(x, w['1']), b['1'])\n","    a1 = f['1'](z1)\n","    z2 = tf.add(tf.matmul(a1, w['2']), b['2'])\n","    a2 = f['2'](z2)\n","    z_out = tf.add(tf.matmul(a2, w['out']), b['out'])\n","    out = f['out'](z_out)\n","\n","    pred = tf.argmax(out, 1)\n","    return pred, z_out\n","\n","opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","\n","for epoch in range(nb_epochs):\n","    epoch_loss = 0\n","    nb_batches = int(len(X_train) / batch_size)\n","    for i in range(nb_batches):\n","        x_batch = X_train[i*batch_size : (i+1)*batch_size, :]\n","        y_batch = y_train[i*batch_size : (i+1)*batch_size]\n","\n","        with tf.GradientTape() as tape:\n","            _, z_out = runNN(x_batch)\n","            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=z_out, labels=y_batch))\n","\n","        gradients = tape.gradient(loss, [w['1'], w['2'], w['out'], b['1'], b['2'], b['out']])\n","        opt.apply_gradients(zip(gradients, [w['1'], w['2'], w['out'], b['1'], b['2'], b['out']]))\n","\n","        epoch_loss += loss\n","\n","    epoch_loss /= nb_batches\n","    print(f'Epoch: {epoch+1}/{nb_epochs}, Avg loss: {epoch_loss:.5f}')\n","\n","pred, _ = runNN(X_test)\n","pred_correct = tf.equal(pred, tf.argmax(y_test, 1))\n","accuracy = tf.reduce_mean(tf.cast(pred_correct, tf.float32))\n","\n","print(f'Test accuracy: {accuracy:.3f}')"]}]}